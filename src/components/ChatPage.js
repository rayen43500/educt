import React, { useState, useEffect, useRef } from 'react';
import { Link } from 'react-router-dom';
import chatService from '../services/chatService';
import WritingPlanner from './WritingPlanner';

// Fonction pour formatter le texte avec mise en √©vidence
const formatMessage = (text) => {
  if (!text) return '';
  
  // Remplacer les mots cl√©s avec des balises de formatage
  const formattedText = text
    // Mettre en gras les mots entre **texte**
    .replace(/\*\*(.*?)\*\*/g, '<strong>$1</strong>')
    // Mettre en italique les mots entre *texte*
    .replace(/\*(.*?)\*/g, '<em>$1</em>')
    // Souligner les mots entre _texte_
    .replace(/_(.*?)_/g, '<u>$1</u>')
    // Colorer les conseils
    .replace(/(Conseil :.*)/g, '<span class="text-success">$1</span>')
    // Colorer les avertissements
    .replace(/(Attention :.*)/g, '<span class="text-warning">$1</span>')
    // Respecter les sauts de ligne
    .replace(/\n/g, '<br/>');
    
  return formattedText;
};

// Fonction s√©curis√©e pour v√©rifier la prise en charge
const safeCheckBrowserCapabilities = () => {
  try {
    const hasBasicFeatures = typeof window !== 'undefined' && 
      window.File && 
      window.FileReader && 
      window.FileList;
    
    // V√©rification s√©curis√©e de Blob
    let hasBlobSupport = false;
    try {
      hasBlobSupport = typeof window.Blob === 'function';
    } catch (e) {
      console.error('Erreur lors de la v√©rification de Blob:', e);
    }
    
    // V√©rification de la prise en charge de la synth√®se vocale
    const hasSpeechSynthesis = typeof window !== 'undefined' && 'speechSynthesis' in window;
    
    // V√©rification de la prise en charge de la reconnaissance vocale
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    const hasSpeechRecognition = typeof SpeechRecognition !== 'undefined';
    
    return {
      hasFileSupport: hasBasicFeatures && hasBlobSupport,
      hasSpeechSynthesis,
      hasSpeechRecognition,
      browserInfo: {
        name: navigator?.userAgent?.match(/chrome|chromium|crios/i) ? 'Chrome' :
              navigator?.userAgent?.match(/firefox|fxios/i) ? 'Firefox' :
              navigator?.userAgent?.match(/safari/i) ? 'Safari' :
              navigator?.userAgent?.match(/edg/i) ? 'Edge' :
              'Autre navigateur',
        version: 'Non d√©tect√©'
      }
    };
  } catch (e) {
    console.error('Erreur lors de la v√©rification du navigateur:', e);
    return { 
      hasFileSupport: false, 
      hasSpeechSynthesis: false,
      hasSpeechRecognition: false,
      browserInfo: { name: 'Erreur', version: 'Erreur' } 
    };
  }
};

const ChatPage = () => {
  // V√©rification initiale s√©curis√©e
  const { hasFileSupport, hasSpeechSynthesis, hasSpeechRecognition, browserInfo } = safeCheckBrowserCapabilities();
  
  const [messages, setMessages] = useState([
    {
      id: 1,
      role: 'system',
      content: "You are ChatAlimi, an educational assistant specialized in correcting and improving student writing. Reply in French and be supportive. Use formatting such as **bold** for important points, *italics* for emphasis, and _underlined_ for corrections. Start guidance with 'Conseil :' and warnings with 'Attention :'. When appropriate, you can send images to illustrate your explanations.\n\nTu dois te concentrer UNIQUEMENT sur la correction de textes. Ne pas proposer d'aide √† la cr√©ation de textes ou de productions √©crites.\n\nVoici les crit√®res d'√©valuation √† utiliser pour analyser et corriger les productions √©crites:\n\nC1 - Ad√©quation avec la situation de communication:\n- Production du nombre de phrases demand√© ou plus\n- Manifestation de compr√©hension en r√©alisant la t√¢che demand√©e\n- Utilisation du vocabulaire appropri√© √† la situation de communication\n- SUGGESTIONS: Proposer des termes plus pr√©cis ou plus adapt√©s au contexte, sugg√©rer d'autres mani√®res d'exprimer les id√©es importantes\n- ID√âES: Sugg√©rer des pistes pour d√©velopper certains aspects du sujet qui m√©riteraient plus d'attention\n\nC2 - Lisibilit√© de l'√©criture:\n- Respect des normes au niveau des lettres minuscules et majuscules\n- SUGGESTIONS: Indiquer o√π ajouter des majuscules avec des exemples pr√©cis\n\nC3 - Correction linguistique:\n- Agencement correct des mots dans les phrases produites\n- Respect des accords √©tudi√©s\n- √âcriture correcte des formes verbales √©tudi√©es\n- Utilisation de la ponctuation forte: point et point d'interrogation\n- SUGGESTIONS: Proposer des reformulations des phrases incorrectes, montrer les formes verbales correctes, proposer des alternatives pour enrichir la ponctuation\n- ID√âES: Proposer des variations de phrases pour exprimer la m√™me id√©e de fa√ßon plus √©l√©gante\n\nC4 - Correction orthographique:\n- √âcriture correcte du lexique et des mots-outils √©tudi√©s\n- SUGGESTIONS: Donner la liste des mots mal orthographi√©s avec leur orthographe correcte, proposer des mots de vocabulaire suppl√©mentaires li√©s au sujet\n- ID√âES: Sugg√©rer un vocabulaire plus riche et des expressions idiomatiques qui enrichiraient le texte\n\nC5 - Coh√©rence du texte:\n- Emploi correct des substituts pour √©viter les r√©p√©titions\n- Progression des √©v√©nements dans le r√©cit\n- Absence de contradiction\n- Introduction opportune de r√©pliques ou passages descriptifs\n- SUGGESTIONS: Proposer des substituts pour remplacer les r√©p√©titions, sugg√©rer des connecteurs logiques pour am√©liorer la progression\n- ID√âES: Sugg√©rer des transitions plus fluides entre les parties du texte, proposer des id√©es pour √©toffer les passages qui manquent de d√©veloppement\n\nC6 - Originalit√© des id√©es:\n- Cr√©ativit√© dans la production\n- Utilisation d'un vocabulaire riche\n- SUGGESTIONS: Proposer des id√©es pour enrichir certains passages, sugg√©rer des expressions imag√©es ou des comparaisons pertinentes\n- ID√âES: Proposer des rebondissements inattendus, des d√©tails surprenants, des dialogues originaux ou des descriptions innovantes pour rendre le texte plus captivant\n\nC7 - Pr√©sentation mat√©rielle:\n- Pr√©sentation d'une copie propre sans rature ni surcharge\n- Respect des caract√©ristiques formelles du type d'√©crit demand√©\n- SUGGESTIONS: Proposer un format adapt√©, sugg√©rer une meilleure disposition du texte si n√©cessaire\n\nPour chaque texte soumis:\n1. Analyse-le en fonction de ces 7 crit√®res\n2. Structure ta r√©ponse avec une section pour chaque crit√®re\n3. Pour chaque crit√®re, identifie les points forts et les points √† am√©liorer\n4. TR√àS IMPORTANT: Donne des suggestions concr√®tes et des exemples d'alternatives pour chaque probl√®me identifi√©\n5. TR√àS IMPORTANT: Propose des id√©es cr√©atives pour enrichir le texte, notamment en d√©veloppant les id√©es qui pourraient √™tre plus approfondies\n6. Termine par une conclusion encourageante avec des conseils d'am√©lioration clairs\n\nNe te contente jamais de signaler les erreurs - propose toujours des alternatives concr√®tes, des corrections sp√©cifiques, des exemples et des id√©es nouvelles pour enrichir la production √©crite.",
      timestamp: new Date().toLocaleTimeString([], { hour: '2-digit', minute: '2-digit' })
    },
    {
      id: 2,
      role: 'assistant',
      content: "Bonjour ! Je suis **ChatAlimi**, ton assistant pour corriger et enrichir tes textes.\n\nPartage ton texte avec moi, et je l'analyserai selon les 7 crit√®res d'√©valuation suivants :\n\n**C1 - Ad√©quation avec la situation de communication**\n**C2 - Lisibilit√© de l'√©criture**\n**C3 - Correction linguistique**\n**C4 - Correction orthographique**\n**C5 - Coh√©rence du texte**\n**C6 - Originalit√© des id√©es**\n**C7 - Pr√©sentation mat√©rielle**\n\nPour chaque crit√®re, je te donnerai :\n‚Ä¢ Les points forts de ton texte\n‚Ä¢ Les √©l√©ments √† am√©liorer\n‚Ä¢ Des suggestions concr√®tes de corrections\n‚Ä¢ Des id√©es cr√©atives pour enrichir ton texte\n\nPar exemple, pour une phrase comme _\"Le gar√ßon a manger une pomme hier.\"_, je pourrais sugg√©rer :\n\n**C3 - Correction linguistique** :\n- ‚úì Points forts : Bonne structure sujet-verbe-compl√©ment\n- ‚ö†Ô∏è √Ä am√©liorer : Conjugaison du verbe \"manger\" au pass√© compos√©\n- üìù Suggestion : \"Le gar√ßon a **mang√©** une pomme hier.\"\n- üí° Id√©e cr√©ative : \"Le jeune gar√ßon a savour√© une pomme juteuse hier apr√®s-midi, se d√©lectant de chaque bouch√©e.\"\n\nTu peux aussi utiliser notre **planificateur de production √©crite** pour organiser tes id√©es avant d'√©crire.\n\nJe suis pr√™t √† t'aider ! Partage ton texte quand tu veux.",
      timestamp: new Date().toLocaleTimeString([], { hour: '2-digit', minute: '2-digit' })
    }
  ]);
  const [newMessage, setNewMessage] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  const [error, setError] = useState(null);
  const messagesEndRef = useRef(null);
  const [selectedImage, setSelectedImage] = useState(null);
  const [imagePreview, setImagePreview] = useState(null);
  const fileInputRef = useRef(null);
  const [isFileUploadEnabled] = useState(hasFileSupport);
  const [browserDetails] = useState(browserInfo);
  
  // Variables d'√©tat pour la reconnaissance vocale et la synth√®se vocale
  const [isListening, setIsListening] = useState(false);
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [currentSpeakingMessageId, setCurrentSpeakingMessageId] = useState(null);
  const [speakingProgress, setSpeakingProgress] = useState(0); // progression de 0 √† 100
  const [isSpeechSynthesisSupported] = useState(hasSpeechSynthesis);
  const [isSpeechRecognitionSupported] = useState(hasSpeechRecognition);
  const [showVoiceSettings, setShowVoiceSettings] = useState(false);
  const [voicePrefs, setVoicePrefs] = useState({
    rate: 0.92,      // Vitesse de lecture (0.1 √† 2)
    pitch: 1.0,      // Ton de la voix (0 √† 2)
    volume: 1.0,     // Volume (0 √† 1)
    useSegmentation: true // Utiliser la segmentation pour une meilleure fiabilit√©
  });
  const recognitionRef = useRef(null);
  const speechSynthesisRef = useRef(hasSpeechSynthesis ? window.speechSynthesis : null);

  // R√©f√©rence pour l'intervalle de v√©rification de la synth√®se vocale
  const checkIntervalRef = useRef(null);

  // Variable d'√©tat pour suivre les interruptions externes
  const [isCancellationRequested, setIsCancellationRequested] = useState(false);
  const activeSpeechRef = useRef({
    isActive: false,
    messageId: null,
    segments: [],
    currentSegment: 0,
    totalSegments: 0
  });

  // Ajouter des r√©f√©rences pour suivre l'√©tat actuel imm√©diatement
  const isSpeakingRef = useRef(false);
  const cancelRequestedRef = useRef(false);

  // Ajout de l'√©tat pour le planificateur d'√©criture
  const [showWritingPlanner, setShowWritingPlanner] = useState(false);

  // useEffect pour les avertissements si n√©cessaire
  useEffect(() => {
    if (!isFileUploadEnabled) {
      console.warn("Ce navigateur ne prend pas en charge toutes les fonctionnalit√©s n√©cessaires pour l'upload d'images.");
      setError(`L'upload d'images n'est pas disponible sur ce navigateur. Si cette fonctionnalit√© est importante, essayez d'utiliser Chrome, Firefox ou Edge.`);
    }
    
    if (!isSpeechSynthesisSupported) {
      console.warn("Ce navigateur ne prend pas en charge la synth√®se vocale.");
    }
    
    if (!isSpeechRecognitionSupported) {
      console.warn("Ce navigateur ne prend pas en charge la reconnaissance vocale.");
    }
  }, [isFileUploadEnabled, isSpeechSynthesisSupported, isSpeechRecognitionSupported]);

  // Corriger le probl√®me de context suspendu de speechSynthesis dans certains navigateurs
  useEffect(() => {
    const fixSpeechSynthesis = () => {
      if (isSpeechSynthesisSupported) {
        // Certains navigateurs suspendent speechSynthesis quand la page devient inactive
        if (speechSynthesisRef.current && speechSynthesisRef.current.paused && !isCancellationRequested) {
          console.log("D√©tection de synth√®se vocale en pause, tentative de reprise...");
          speechSynthesisRef.current.resume();
        }
      }
    };

    // Ajouter des √©couteurs d'√©v√©nements pour d√©tecter les changements de visibilit√©
    document.addEventListener('visibilitychange', fixSpeechSynthesis);
    window.addEventListener('focus', fixSpeechSynthesis);
    window.addEventListener('blur', fixSpeechSynthesis);

    // Intervalles de v√©rification et correction pour les navigateurs probl√©matiques
    const resumeInterval = setInterval(() => {
      if (isSpeaking && !isCancellationRequested) {
        fixSpeechSynthesis();
      }
    }, 1000);

    return () => {
      document.removeEventListener('visibilitychange', fixSpeechSynthesis);
      window.removeEventListener('focus', fixSpeechSynthesis);
      window.removeEventListener('blur', fixSpeechSynthesis);
      clearInterval(resumeInterval);
    };
  }, [isSpeaking, isSpeechSynthesisSupported, isCancellationRequested]);

  // useEffect pour v√©rifier p√©riodiquement si la synth√®se vocale est toujours active
  useEffect(() => {
    let checkInterval;
    
    if (isSpeaking) {
      // V√©rifier toutes les 500ms si la synth√®se vocale est toujours active
      checkInterval = setInterval(() => {
        if (speechSynthesisRef.current && !speechSynthesisRef.current.speaking) {
          // Si la synth√®se vocale s'est arr√™t√©e sans d√©clencher onend
          setIsSpeaking(false);
          setCurrentSpeakingMessageId(null);
          setSpeakingProgress(0);
        }
      }, 500);
    }
    
    return () => {
      if (checkInterval) {
        clearInterval(checkInterval);
      }
    };
  }, [isSpeaking]);

  const scrollToBottom = () => {
    try {
      messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });
    } catch (err) {
      console.error('Erreur lors du d√©filement vers le bas:', err);
      // Fallback silencieux en cas d'erreur
    }
  };

  useEffect(() => {
    try {
      scrollToBottom();
    } catch (err) {
      console.error('Erreur lors du d√©filement automatique:', err);
    }
  }, [messages]);

  const handleImageChange = (e) => {
    if (!isFileUploadEnabled) {
      const browser = browserDetails.name || browserInfo.name;
      setError(`Votre navigateur (${browser}) ne prend pas en charge l'upload d'images. Veuillez utiliser un navigateur plus r√©cent comme Chrome, Firefox ou Edge.`);
      return;
    }

    const file = e.target.files[0];
    console.log("Fichier s√©lectionn√©:", file);
    
    if (!file) {
      console.log("Aucun fichier s√©lectionn√©");
      return;
    }
    
    // V√©rifier le type de fichier
    if (!file.type.startsWith('image/')) {
      setError(`Le fichier s√©lectionn√© (${file.type}) n'est pas une image. Types accept√©s: jpg, png, gif, webp.`);
      console.log("Type de fichier non valide:", file.type);
      return;
    }
    
    if (file.size > 5 * 1024 * 1024) {
      setError(`L'image est trop volumineuse (${(file.size / 1024 / 1024).toFixed(2)} Mo). Maximum 5 Mo.`);
      console.log("Fichier trop volumineux:", file.size);
      return;
    }
    
    try {
      setSelectedImage(file);
      
      // Cr√©er un aper√ßu de l'image
      const reader = new FileReader();
      
      reader.onloadend = () => {
        console.log("Image charg√©e avec succ√®s");
        setImagePreview(reader.result);
      };
      
      reader.onerror = (error) => {
        console.error("Erreur lors de la lecture du fichier:", error);
        setError(`Impossible de lire le fichier image (${file.name}). Erreur: ${error.message || 'inconnue'}`);
      };
      
      reader.readAsDataURL(file);
    } catch (err) {
      console.error("Erreur lors du traitement de l'image:", err);
      setError(`Une erreur s'est produite lors du traitement de l'image: ${err.message || 'Erreur inconnue'}`);
    }
  };

  const clearSelectedImage = () => {
    setSelectedImage(null);
    setImagePreview(null);
    if (fileInputRef.current) {
      fileInputRef.current.value = "";
    }
  };

  const triggerImageSelect = () => {
    if (!isFileUploadEnabled) {
      const browser = browserDetails.name || browserInfo.name;
      const version = browserDetails.version || browserInfo.version;
      setError(`Votre navigateur (${browser} ${version}) ne prend pas en charge l'upload d'images. Veuillez utiliser Chrome, Firefox ou Edge.`);
      return;
    }
    
    try {
      // V√©rifier si fileInputRef.current existe avant d'appeler click()
      if (fileInputRef.current) {
        fileInputRef.current.click();
      } else {
        console.error("R√©f√©rence au input file manquante");
        setError("Impossible d'ouvrir le s√©lecteur de fichiers. Veuillez actualiser la page ou utiliser un autre navigateur.");
      }
    } catch (err) {
      console.error("Erreur lors de l'ouverture du s√©lecteur de fichiers:", err);
      setError(`Erreur lors de l'ouverture du s√©lecteur de fichiers: ${err.message || 'Erreur inconnue'}`);
    }
  };

  const handleSend = async (e) => {
    e.preventDefault();
    
    console.log("Envoi du message - Texte:", newMessage);
    console.log("Envoi du message - Image:", selectedImage);
    
    if (newMessage.trim() === '' && !selectedImage) {
      console.log("Aucun contenu √† envoyer");
      return;
    }
    
    const timestamp = new Date().toLocaleTimeString([], { hour: '2-digit', minute: '2-digit' });
    
    // Add user message with image if selected
    const userMessage = {
      id: messages.length + 1,
      role: 'user',
      content: newMessage,
      timestamp: timestamp,
      image: imagePreview
    };
    
    console.log("Message utilisateur cr√©√©:", userMessage);
    
    setMessages(prevMessages => [...prevMessages, userMessage]);
    setNewMessage('');
    setIsLoading(true);
    setError(null);
    clearSelectedImage();
    
    try {
      // Pr√©parer un message pour l'API qui d√©crit l'image si pr√©sente
      let messageForAPI = newMessage;
      
      if (userMessage.image) {
        console.log("Image d√©tect√©e dans le message");
        messageForAPI += "\n\n[L'utilisateur a joint une image √† ce message]";
      }
      
      // Format messages for the API (only include role and content)
      const apiMessages = messages
        .filter(msg => msg.role === 'system' || msg.role === 'user' || msg.role === 'assistant')
        .map(({ role, content }) => ({ role, content }));
      
      // Add the new user message
      apiMessages.push({ role: userMessage.role, content: messageForAPI });
      
      // V√©rifier si la demande semble √™tre une correction de texte
      const isCorrectionRequest = messageForAPI.toLowerCase().includes('corrige') || 
                                  messageForAPI.toLowerCase().includes('correction') ||
                                  messageForAPI.toLowerCase().includes('√©valuer') ||
                                  messageForAPI.toLowerCase().includes('crit√®res');
      
      // Si c'est une demande de correction, ajouter un message syst√®me sp√©cifique
      if (isCorrectionRequest) {
        apiMessages.push({
          role: 'system',
          content: "Pour cette correction, analyse le texte de l'√©l√®ve selon les 7 crit√®res d'√©valuation mentionn√©s pr√©c√©demment. Structure ta r√©ponse en 7 sections, une pour chaque crit√®re. Pour chaque crit√®re, identifie les points forts et les points √† am√©liorer. Utilise des exemples concrets tir√©s du texte de l'√©l√®ve. Termine par une conclusion encourageante et constructive. Utilise le formatage (gras, italique, soulign√©) pour mettre en √©vidence les √©l√©ments importants."
        });
      }
      
      console.log("Appel de l'API avec les messages:", apiMessages);
      
      // Get response from AI
      const response = await chatService.getChatResponse(apiMessages);
      
      console.log("R√©ponse re√ßue de l'API:", response);
      
      // Add bot response - handle both text-only and multimodal responses
      const botResponse = {
        id: messages.length + 2,
        role: 'assistant',
        timestamp: new Date().toLocaleTimeString([], { hour: '2-digit', minute: '2-digit' })
      };
      
      // Check if response is a multimodal object
      if (typeof response === 'object' && response !== null && response.text !== undefined) {
        // C'est une r√©ponse multimodale avec potentiellement des images
        botResponse.content = response.text;
        
        // Ajouter les images s'il y en a
        if (response.images && response.images.length > 0) {
          botResponse.images = response.images;
        }
      } else {
        // R√©ponse texte standard
        botResponse.content = response;
      }
      
      setMessages(prevMessages => [...prevMessages, botResponse]);
    } catch (err) {
      console.error('Error getting chat response:', err);
      // Utiliser un message d'erreur par d√©faut si l'erreur ou le message d'erreur est ind√©fini
      const errorMessage = err?.message || "Une erreur s'est produite lors de la communication avec l'IA";
      setError(`D√©sol√©, ${errorMessage}`);
    } finally {
      setIsLoading(false);
    }
  };

  // Fonction pour lire √† haute voix un message
  const handleSpeakMessage = (text, messageId) => {
    if (!isSpeechSynthesisSupported) {
      setError('La synth√®se vocale n\'est pas prise en charge par votre navigateur. Essayez Chrome ou Edge.');
      return;
    }
    
    if (isSpeaking) {
      // Ajout d'un log pour identifier d'o√π vient l'annulation
      console.log("Annulation demand√©e de la lecture en cours. Source: handleSpeakMessage");
      
      // Marquer comme une annulation explicite de l'utilisateur
      setIsCancellationRequested(true);
      cancelRequestedRef.current = true;
      
      // Arr√™ter la lecture en cours
      try {
        speechSynthesisRef.current.cancel();
        console.log("Annulation r√©ussie de la synth√®se vocale");
      } catch (err) {
        console.error("Erreur lors de l'annulation de la synth√®se vocale:", err);
      }
      
      setIsSpeaking(false);
      isSpeakingRef.current = false;
      setCurrentSpeakingMessageId(null);
      setSpeakingProgress(0);
      
      // R√©initialiser la r√©f√©rence de suivi
      activeSpeechRef.current = {
        isActive: false,
        messageId: null,
        segments: [],
        currentSegment: 0,
        totalSegments: 0
      };
      
      if (checkIntervalRef.current) clearInterval(checkIntervalRef.current);
      
      // R√©initialiser l'√©tat d'annulation apr√®s un court d√©lai
      setTimeout(() => {
        setIsCancellationRequested(false);
        cancelRequestedRef.current = false;
        console.log("√âtat d'annulation r√©initialis√©");
      }, 500);
      
      return;
    }
    
    // Initialiser l'√©tat de lecture imm√©diatement avec les r√©f√©rences
    setIsSpeaking(true);
    isSpeakingRef.current = true;
    setCurrentSpeakingMessageId(messageId);
    setIsCancellationRequested(false);
    cancelRequestedRef.current = false;
    
    try {
      // Force activer l'audio sur l'appareil avec un son silencieux
      // Cela est n√©cessaire sur certains navigateurs/appareils
      try {
        // Cr√©er un court son pour activer l'audio du syst√®me
        const audioContext = new (window.AudioContext || window.webkitAudioContext)();
        const oscillator = audioContext.createOscillator();
        const gainNode = audioContext.createGain();
        
        // Configurer un son tr√®s court et √† peine audible
        gainNode.gain.value = 0.05; // Volume suffisant pour activer l'audio
        oscillator.frequency.value = 440; // Fr√©quence audible (La 440Hz)
        oscillator.connect(gainNode);
        gainNode.connect(audioContext.destination);
        
        // Jouer le son pendant 100ms
        oscillator.start();
        setTimeout(() => {
          oscillator.stop();
          audioContext.close().catch(err => console.log("Erreur fermeture audio:", err));
        }, 100);
        
        // Forcer le d√©verrouillage des contr√¥les audio sur iOS et Safari
        if (typeof document !== 'undefined') {
          const unlockAudio = () => {
            if (audioContext.state === 'suspended') {
              audioContext.resume().then(() => {
                console.log('Audio context d√©verrouill√© avec succ√®s');
              }).catch(err => {
                console.warn('√âchec du d√©verrouillage audio:', err);
              });
            }
          };
          
          // Attacher des √©couteurs d'√©v√©nements temporaires pour le d√©verrouillage
          const events = ['touchstart', 'touchend', 'mousedown', 'keydown'];
          const cleanupFuncs = events.map(event => {
            const handler = unlockAudio;
            document.addEventListener(event, handler, { once: true });
            return () => document.removeEventListener(event, handler);
          });
          
          // Nettoyer les √©couteurs apr√®s 5 secondes
          setTimeout(() => {
            cleanupFuncs.forEach(cleanup => cleanup());
          }, 5000);
        }
      } catch (err) {
        console.warn("Impossible d'activer l'audio du syst√®me:", err);
        // On continue malgr√© l'erreur, car cela pourrait quand m√™me fonctionner
      }
      
      // Nettoyer le texte pour la synth√®se vocale
      // 1. Supprimer les balises HTML et entit√©s HTML
      let cleanText = text.replace(/<\/?[^>]+(>|$)/g, " ");
      cleanText = cleanText.replace(/&[a-z]+;/g, " ");
      
      // 2. Remplacer les caract√®res sp√©ciaux de formatage par des espaces
      cleanText = cleanText.replace(/[\*_\[\]\(\)\{\}\#\+\~\`]/g, " ");
      
      // 3. Conserver la ponctuation essentielle et les accents pour une lecture correcte
      cleanText = cleanText.replace(/[^\w\s.,!?;:¬´¬ª""''\-‚Äì‚Äî√†√¢√§√©√®√™√´√Æ√Ø√¥√∂√π√ª√º√ø√ß√Ä√Ç√Ñ√â√à√ä√ã√é√è√î√ñ√ô√õ√ú≈∏√á]/g, " ");
      
      // 4. Supprimer les espaces multiples et les espaces avant la ponctuation
      cleanText = cleanText.replace(/\s+/g, " ");
      cleanText = cleanText.replace(/\s+([.,!?;:])/g, "$1");
      cleanText = cleanText.trim();
      
      // 5. Am√©liorations sp√©cifiques pour la lecture
      // Ajouter une pause apr√®s les points pour une meilleure s√©paration
      cleanText = cleanText.replace(/\./g, ". ");
      // Ajouter une pause apr√®s les questions/exclamations
      cleanText = cleanText.replace(/[!?]/g, "$& ");
      
      console.log("Texte original:", text);
      console.log("Texte nettoy√© pour lecture:", cleanText);
      
      // Marquer le message comme "en cours de lecture"
      setIsSpeaking(true);
      isSpeakingRef.current = true;
      setCurrentSpeakingMessageId(messageId);
      setSpeakingProgress(0);
      
      // Annuler toute lecture pr√©c√©dente
      speechSynthesisRef.current.cancel();
      
      // Obtenir les voix disponibles
      const getVoices = () => {
        // R√©cup√©rer les voix disponibles
        const voices = speechSynthesisRef.current.getVoices();
        
        // Chercher une voix fran√ßaise
        let frenchVoice = null;
        
        for (const voice of voices) {
          // Priorit√© aux voix fran√ßaises
          if (voice.lang.includes('fr') || voice.lang.includes('FR')) {
            frenchVoice = voice;
            // Priorit√© aux voix f√©minines pour plus de clart√©
            if (voice.name.includes('female') || voice.name.toLowerCase().includes('femme')) {
              break;
            }
          }
        }
        
        return frenchVoice;
      };
      
      // Solution par lecture de segments courts pour garantir que tout le texte est lu
      const readTextInSegments = () => {
        // Diviser le texte en segments plus courts et g√©rables
        const words = cleanText.split(/\s+/);
        let segments = [];
        
        // D√©tecter le navigateur pour adapter les strat√©gies
        const isEdge = navigator.userAgent.indexOf("Edg") > -1;
        const isSafari = /^((?!chrome|android).)*safari/i.test(navigator.userAgent);
        
        // Adapter la longueur des segments selon le navigateur
        const wordsPerSegment = (isEdge || isSafari) ? 5 : 7; // Segments encore plus courts pour Edge et Safari
        
        console.log("Navigateur d√©tect√©:", isEdge ? "Edge" : isSafari ? "Safari" : "Autre", "- Segments de", wordsPerSegment, "mots");
        
        // Diviser le texte en segments
        for (let i = 0; i < words.length; i += wordsPerSegment) {
          const segmentCandidate = words.slice(i, i + wordsPerSegment).join(" ").trim();
          // N'ajouter que les segments non vides
          if (segmentCandidate) {
            segments.push(segmentCandidate);
          }
        }
        
        console.log("Segments √† lire:", segments.length, segments);
        
        // Le nombre total de segments pour calculer la progression
        const totalSegments = segments.length;
        
        // Mettre √† jour la r√©f√©rence de suivi
        activeSpeechRef.current = {
          isActive: true,
          messageId: messageId,
          segments: segments,
          currentSegment: 0,
          totalSegments: totalSegments,
          lastActivity: Date.now()
        };
        
        // R√©initialiser l'√©tat d'annulation
        setIsCancellationRequested(false);
        
        // Fonction pour lire chaque segment s√©quentiellement
        const readNextSegment = (index = 0) => {
          // V√©rifier si la lecture doit se poursuivre en utilisant les r√©f√©rences
          if (cancelRequestedRef.current || !isSpeakingRef.current || index >= totalSegments) {
            console.log("Lecture termin√©e ou annul√©e. isSpeaking:", isSpeakingRef.current, "isCancellationRequested:", cancelRequestedRef.current, "index:", index, "totalSegments:", totalSegments);
            
            // Ne pas modifier l'√©tat si une annulation explicite a d√©j√† √©t√© effectu√©e
            if (!cancelRequestedRef.current) {
              setIsSpeaking(false);
              isSpeakingRef.current = false;
              setCurrentSpeakingMessageId(null);
              setSpeakingProgress(100);
            }
            
            // Mettre √† jour la r√©f√©rence de suivi
            activeSpeechRef.current.isActive = false;
            activeSpeechRef.current.currentSegment = 0;
            
            if (checkIntervalRef.current) clearInterval(checkIntervalRef.current);
            return;
          }
          
          // Mettre √† jour la progression
          const progress = Math.floor((index / totalSegments) * 100);
          setSpeakingProgress(progress);
          
          // Mettre √† jour le segment en cours dans la r√©f√©rence
          activeSpeechRef.current.currentSegment = index;
          
          // Cr√©er l'utterance pour ce segment
          const utterance = new SpeechSynthesisUtterance(segments[index]);
          
          // Base configuration pour tous les navigateurs
          utterance.lang = 'fr-FR';
          
          // R√©cup√©rer une voix fran√ßaise si disponible
          const frenchVoice = getVoices();
          if (frenchVoice) {
            utterance.voice = frenchVoice;
          } else {
            // Si aucune voix fran√ßaise n'est trouv√©e, on force la langue
            utterance.lang = 'fr-FR';
          }
          
          // D√©tecter le navigateur pour adapter les strat√©gies
          const isEdge = navigator.userAgent.indexOf("Edg") > -1;
          const isSafari = /^((?!chrome|android).)*safari/i.test(navigator.userAgent);
          
          // Appliquer les pr√©f√©rences utilisateur avec volume maximis√©
          utterance.rate = voicePrefs.rate;
          if (isEdge || isSafari) {
            // Edge et Safari ont des probl√®mes avec certains taux de lecture
            utterance.rate = Math.min(Math.max(voicePrefs.rate, 0.7), 1.1); // Restreindre la plage
          }
          
          utterance.pitch = voicePrefs.pitch;
          utterance.volume = 1.0; // Toujours volume maximum pour √©viter les probl√®mes d'audio
          
          // Dans Safari, utiliser une pause plus longue entre les segments
          const pauseDuration = isSafari ? 300 : isEdge ? 200 : 150;
          
          console.log(`Lecture du segment ${index + 1}/${totalSegments}: "${segments[index]}"`);
          
          // G√©rer la fin de la lecture d'un segment
          utterance.onstart = () => {
            console.log(`D√©but de lecture du segment ${index + 1}`);
            activeSpeechRef.current.lastActivity = Date.now();
          };
          
          utterance.onend = () => {
            console.log(`Fin du segment ${index + 1}, isSpeaking:`, isSpeakingRef.current);
            
            // Si l'√©tat a chang√© entretemps, g√©rer proprement l'arr√™t
            if (!isSpeakingRef.current) {
              console.log(`Le segment ${index + 1} s'est termin√© mais la lecture a √©t√© annul√©e entre-temps`);
              return;
            }
            
            // Petite pause avant le prochain segment pour une meilleure fluidit√©
            const timeoutId = setTimeout(() => {
              // Double v√©rification pour √©viter des appels apr√®s annulation
              if (isSpeakingRef.current && !cancelRequestedRef.current) {
                // V√©rifier que la r√©f√©rence est toujours valide
                if (activeSpeechRef.current.isActive && activeSpeechRef.current.messageId === messageId) {
                  try {
                    readNextSegment(index + 1);
                  } catch (err) {
                    console.error(`Erreur lors de la lecture du segment ${index + 1}:`, err);
                    // Tenter de continuer malgr√© l'erreur apr√®s un d√©lai plus long
                    setTimeout(() => {
                      if (isSpeakingRef.current && !cancelRequestedRef.current) {
                        try {
                          readNextSegment(index + 1);
                        } catch (e) {
                          console.error("Deuxi√®me √©chec de lecture:", e);
                          setIsSpeaking(false);
                          isSpeakingRef.current = false;
                          setError("Impossible de continuer la lecture apr√®s plusieurs tentatives.");
                        }
                      }
                    }, 1000);
                  }
                } else {
                  console.log("La r√©f√©rence de suivi a chang√©, arr√™t de la s√©quence de lecture");
                }
              } else {
                console.log("La lecture a √©t√© annul√©e pendant la pause entre segments");
              }
            }, pauseDuration);
            
            // Stocker l'ID du timeout pour pouvoir l'annuler si n√©cessaire
            const timeouts = activeSpeechRef.current.timeouts || [];
            activeSpeechRef.current.timeouts = [...timeouts, timeoutId];
          };
          
          // G√©rer les erreurs lors de la lecture d'un segment
          utterance.onerror = (event) => {
            console.error("Erreur de lecture pour le segment", index, event);
            console.log("Type d'erreur:", event.error, "Message:", event.message);
            
            // Si c'est l'erreur "canceled", possiblement due √† une annulation directe
            if (event.error === 'canceled' || event.error === 'interrupted') {
              console.log("Annulation d√©tect√©e, v√©rification si elle √©tait intentionnelle...");
              
              // Si ce n'√©tait pas une annulation explicite, tenter de r√©cup√©rer
              if (!cancelRequestedRef.current && isSpeakingRef.current) {
                console.log("Annulation non intentionnelle d√©tect√©e, tentative de reprise...");
                
                // Attendre un peu plus longtemps avant de r√©essayer
                setTimeout(() => {
                  if (isSpeakingRef.current && !cancelRequestedRef.current) {
                    // Continuer avec le segment actuel ou le suivant selon le contexte
                    try {
                      readNextSegment(index);
                    } catch (err) {
                      console.error("Impossible de reprendre apr√®s annulation:", err);
                    }
                  }
                }, 800);
              } else {
                console.log("Annulation intentionnelle confirm√©e, arr√™t de la lecture");
              }
              return;
            }
            
            // Pour les autres types d'erreurs, continuer avec le segment suivant
            if (isSpeakingRef.current && !cancelRequestedRef.current) {
              console.warn(`Erreur, passage au segment ${index + 2}`);
              setTimeout(() => {
                if (isSpeakingRef.current && !cancelRequestedRef.current) {
                  try {
                    readNextSegment(index + 1);
                  } catch (err) {
                    console.error("Impossible de continuer apr√®s erreur:", err);
                  }
                }
              }, pauseDuration * 2); // Pause plus longue apr√®s une erreur
            }
          };
          
          // Lire le segment
          speechSynthesisRef.current.speak(utterance);
          
          // Force le navigateur √† jouer le son imm√©diatement
          if (!speechSynthesisRef.current.speaking) {
            speechSynthesisRef.current.pause();
            speechSynthesisRef.current.resume();
          }
        };
        
        // Commencer la lecture s√©quentielle
        if (segments.length > 0) {
          // S'assurer que l'√©tat est initialis√© AVANT le setTimeout
          // Utiliser des r√©f√©rences pour un acc√®s imm√©diat
          
          // Affirmer explicitement l'√©tat de lecture avant le timeout
          if (!isSpeakingRef.current) {
            console.log("Initialisation explicite de l'√©tat de lecture");
            setIsSpeaking(true);
            isSpeakingRef.current = true;
            setCurrentSpeakingMessageId(messageId);
          }
          
          // R√©initialiser explicitement l'√©tat d'annulation
          if (cancelRequestedRef.current) {
            console.log("R√©initialisation de l'√©tat d'annulation avant d√©marrage");
            setIsCancellationRequested(false);
            cancelRequestedRef.current = false;
          }
          
          // Attendre un court instant avant de d√©marrer la lecture
          console.log("Programmation du d√©marrage de lecture dans 300ms");
          setTimeout(() => {
            // Double v√©rification explicite de l'√©tat en utilisant les r√©f√©rences
            console.log("V√©rification avant d√©marrage - isSpeaking:", isSpeakingRef.current, "isCancellationRequested:", cancelRequestedRef.current);
            
            // V√©rifier que la lecture n'a pas √©t√© annul√©e entre-temps
            if (!isSpeakingRef.current) {
              console.log("√âtat de lecture non d√©fini, d√©finition explicite");
              setIsSpeaking(true);
              isSpeakingRef.current = true;
              setCurrentSpeakingMessageId(messageId);
            }
            
            if (cancelRequestedRef.current) {
              console.log("Annulation demand√©e pendant l'initialisation, abandon");
              setIsSpeaking(false);
              isSpeakingRef.current = false;
              console.log("La lecture a √©t√© annul√©e avant le d√©but de la s√©quence");
            } else {
              // V√©rifier explicitement que la synth√®se vocale est disponible
              if (speechSynthesisRef.current) {
                console.log("Synth√®se vocale disponible, d√©marrage de la lecture");
                try {
                  readNextSegment(0);
                } catch (err) {
                  console.error("Erreur au d√©marrage de la lecture:", err);
                  setIsSpeaking(false);
                  isSpeakingRef.current = false;
                  setError("Impossible de d√©marrer la lecture vocale: " + (err.message || "erreur inconnue"));
                }
              } else {
                console.error("R√©f√©rence √† la synth√®se vocale perdue, tentative de r√©cup√©ration");
                // Tenter de r√©cup√©rer la r√©f√©rence
                speechSynthesisRef.current = window.speechSynthesis;
                if (speechSynthesisRef.current) {
                  console.log("R√©cup√©ration r√©ussie, d√©marrage de la lecture");
                  try {
                    readNextSegment(0);
                  } catch (e) {
                    console.error("Erreur au d√©marrage apr√®s r√©cup√©ration:", e);
                    setIsSpeaking(false);
                    isSpeakingRef.current = false;
                    setError("Erreur lors de l'initialisation de la synth√®se vocale");
                  }
                } else {
                  console.error("Impossible de r√©cup√©rer la synth√®se vocale");
                  setIsSpeaking(false);
                  isSpeakingRef.current = false;
                  setError("La synth√®se vocale n'est plus disponible. Veuillez actualiser la page.");
                }
              }
            }
          }, 300);
        } else {
          // S'il n'y a pas de segments valides (texte vide ou nettoy√© en vide)
          console.log("Aucun segment valide √† lire.");
          setIsSpeaking(false);
          isSpeakingRef.current = false;
          setCurrentSpeakingMessageId(null);
          setSpeakingProgress(100);
          activeSpeechRef.current.isActive = false;
        }
        
        // Surveiller si la lecture s'arr√™te de mani√®re inattendue
        // Nettoyer l'intervalle pr√©c√©dent s'il existe
        if (checkIntervalRef.current) {
          console.log("Nettoyage de l'intervalle de v√©rification pr√©c√©dent");
          clearInterval(checkIntervalRef.current);
        }
        
        checkIntervalRef.current = setInterval(() => {
          if (!isSpeakingRef.current || cancelRequestedRef.current) {
            console.log("Arr√™t de la surveillance: plus en mode lecture ou annulation demand√©e");
            clearInterval(checkIntervalRef.current);
            return;
          }
          
          if (!speechSynthesisRef.current.speaking && isSpeakingRef.current && !cancelRequestedRef.current) {
            console.log("La lecture s'est arr√™t√©e de mani√®re inattendue, tentative de reprise...");
            console.log("√âtat actuel - speaking:", speechSynthesisRef.current.speaking, "paused:", speechSynthesisRef.current.paused);
            
            // Tenter de reprendre la lecture l√† o√π elle s'est arr√™t√©e en utilisant la r√©f√©rence de suivi
            const currentIndex = activeSpeechRef.current.currentSegment;
            
            // V√©rifier que l'on est bien dans le bon contexte de lecture
            if (activeSpeechRef.current.isActive && 
                activeSpeechRef.current.messageId === messageId &&
                currentIndex < activeSpeechRef.current.totalSegments) {
              
              console.log(`Reprise √† partir du segment ${currentIndex + 1}/${activeSpeechRef.current.totalSegments}`);
              speechSynthesisRef.current.cancel(); // Annuler tout ce qui pourrait √™tre en attente
              
              // Attendre un court instant avant la reprise
              setTimeout(() => {
                if (isSpeakingRef.current && !cancelRequestedRef.current) { // V√©rifier encore si toujours en mode lecture
                  readNextSegment(currentIndex);
                } else {
                  console.log("La lecture a √©t√© annul√©e pendant la tentative de reprise");
                }
              }, 200);
            } else {
              console.log("Impossible de reprendre: r√©f√©rences de suivi invalides ou lecture termin√©e");
              clearInterval(checkIntervalRef.current);
            }
            
            // Il faut arr√™ter l'intervalle pour √©viter des reprises multiples
            clearInterval(checkIntervalRef.current);
          }
        }, 2000);
      };
      
      // Attendre que les voix soient charg√©es si n√©cessaire
      if (speechSynthesisRef.current.getVoices().length === 0) {
        // Essayer de forcer le chargement des voix
        speechSynthesisRef.current.onvoiceschanged = () => {
          console.log("Voix charg√©es:", speechSynthesisRef.current.getVoices().length);
          // D√©marrer la lecture par segments une fois les voix charg√©es
          readTextInSegments();
          speechSynthesisRef.current.onvoiceschanged = null;
        };
        
        // Si apr√®s 2 secondes les voix ne sont toujours pas charg√©es, tenter quand m√™me la lecture
        setTimeout(() => {
          if (speechSynthesisRef.current.getVoices().length === 0) {
            console.warn("Impossible de charger les voix, tentative de lecture sans voix sp√©cifique");
            readTextInSegments();
          }
        }, 2000);
      } else {
        console.log("Voix disponibles:", speechSynthesisRef.current.getVoices().length);
        // Voix d√©j√† charg√©es, d√©marrer imm√©diatement
        readTextInSegments();
      }
    } catch (err) {
      console.error('Erreur lors de la synth√®se vocale:', err);
      setIsSpeaking(false);
      isSpeakingRef.current = false;
      setCurrentSpeakingMessageId(null);
      setSpeakingProgress(0);
      setError(`Probl√®me de lecture audio: ${err.message || 'erreur inconnue'}. V√©rifiez le volume de votre appareil.`);
    }
  };
  
  // Fonction pour d√©marrer/arr√™ter la reconnaissance vocale
  const startSpeechRecognition = () => {
    if (!isSpeechRecognitionSupported) {
      setError('La reconnaissance vocale n\'est pas prise en charge par votre navigateur. Essayez Chrome ou Edge.');
      return;
    }
    
    // D√©finir la classe de reconnaissance vocale appropri√©e
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    
    if (isListening) {
      // Arr√™ter la reconnaissance en cours
      if (recognitionRef.current) {
        recognitionRef.current.stop();
      }
      return;
    }
    
    try {
      // Cr√©er une nouvelle instance de reconnaissance vocale
      recognitionRef.current = new SpeechRecognition();
      
      // Configurer la reconnaissance
      recognitionRef.current.lang = 'fr-FR';
      recognitionRef.current.continuous = false;
      recognitionRef.current.interimResults = true;
      
      // Indiquer √† l'utilisateur que l'enregistrement a commenc√©
      setError(null); // Effacer les messages d'erreur pr√©c√©dents
      
      // G√©rer les r√©sultats
      recognitionRef.current.onresult = (event) => {
        const transcript = event.results[0][0].transcript;
        const isFinal = event.results[0].isFinal;
        
        // Afficher le texte reconnu uniquement lorsqu'il est finalis√©
        if (isFinal) {
          // Traiter le texte reconnu (premi√®re lettre en majuscule si c'est le d√©but d'un message)
          let processedText = transcript;
          if (newMessage.trim() === '') {
            processedText = transcript.charAt(0).toUpperCase() + transcript.slice(1);
          }
          
          setNewMessage((prev) => {
            const updatedMessage = prev.trim() === '' ? 
              processedText : 
              `${prev} ${processedText}`;
            return updatedMessage;
          });
        }
      };
      
      // G√©rer les √©v√©nements de d√©but et de fin
      recognitionRef.current.onstart = () => {
        setIsListening(true);
      };
      
      recognitionRef.current.onend = () => {
        setIsListening(false);
      };
      
      // G√©rer les erreurs
      recognitionRef.current.onerror = (event) => {
        console.error('Erreur de reconnaissance vocale:', event);
        setIsListening(false);
        
        // Messages d'erreur plus descriptifs
        if (event.error === 'no-speech') {
          setError('Aucune parole d√©tect√©e. Veuillez parler plus fort ou v√©rifier votre microphone.');
        } else if (event.error === 'audio-capture') {
          setError('Impossible d\'acc√©der au microphone. V√©rifiez les permissions de votre navigateur.');
        } else if (event.error === 'not-allowed') {
          setError('L\'acc√®s au microphone a √©t√© refus√©. V√©rifiez les permissions de votre navigateur.');
        } else {
          setError(`Erreur lors de la reconnaissance vocale: ${event.error}`);
        }
      };
      
      // D√©marrer la reconnaissance
      recognitionRef.current.start();
    } catch (err) {
      console.error('Erreur lors de l\'initialisation de la reconnaissance vocale:', err);
      setError(`La reconnaissance vocale n'est pas disponible ou une erreur s'est produite: ${err.message}`);
    }
  };

  // Nettoyage lors de l'annulation explicite
  const cancelSpeech = () => {
    // Annuler tous les timeouts en attente
    if (activeSpeechRef.current.timeouts && activeSpeechRef.current.timeouts.length > 0) {
      activeSpeechRef.current.timeouts.forEach(timeoutId => {
        clearTimeout(timeoutId);
      });
      activeSpeechRef.current.timeouts = [];
    }
    
    // Marquer l'annulation comme intentionnelle avec les deux m√©canismes
    setIsCancellationRequested(true);
    cancelRequestedRef.current = true;
    
    // Nettoyer l'intervalle de v√©rification
    if (checkIntervalRef.current) {
      clearInterval(checkIntervalRef.current);
      checkIntervalRef.current = null;
    }
    
    // Arr√™ter la synth√®se vocale
    try {
      speechSynthesisRef.current.cancel();
    } catch (err) {
      console.error("Erreur lors de l'annulation de la synth√®se vocale:", err);
    }
    
    // R√©initialiser les √©tats avec les deux m√©canismes
    setIsSpeaking(false);
    isSpeakingRef.current = false;
    setCurrentSpeakingMessageId(null);
    setSpeakingProgress(0);
    
    // R√©initialiser la r√©f√©rence
    activeSpeechRef.current = {
      isActive: false,
      messageId: null,
      segments: [],
      currentSegment: 0,
      totalSegments: 0,
      timeouts: []
    };
    
    // R√©initialiser l'√©tat d'annulation apr√®s un d√©lai
    setTimeout(() => {
      setIsCancellationRequested(false);
      cancelRequestedRef.current = false;
    }, 500);
  };

  // Ajouter un useEffect pour cr√©er un "watchdog" qui garde la lecture active
  useEffect(() => {
    // Ne rien faire si la synth√®se vocale n'est pas support√©e
    if (!isSpeechSynthesisSupported) return;
    
    // Si on est suppos√© √™tre en train de lire
    if (isSpeakingRef.current && !cancelRequestedRef.current) {
      let watchdogTimer = null;
      
      // Mettre en place un watchdog qui v√©rifie r√©guli√®rement l'√©tat de la synth√®se vocale
      const watchdog = () => {
        if (!isSpeakingRef.current) {
          // Si la r√©f√©rence indique qu'on ne parle plus, arr√™ter le watchdog
          console.log("Watchdog: L'√©tat de parole a chang√©, arr√™t de la surveillance");
          return;
        }
        
        // Si la synth√®se est suppos√©e √™tre active mais ne l'est pas
        if (activeSpeechRef.current.isActive && !speechSynthesisRef.current.speaking) {
          // V√©rifier s'il s'agit d'une pause normale entre segments
          const timeSinceLastActivity = Date.now() - (activeSpeechRef.current.lastActivity || 0);
          console.log(`Watchdog: V√©rification d'activit√©, temps √©coul√©: ${timeSinceLastActivity}ms`);
          
          // Si √ßa fait plus de 800ms qu'il n'y a pas eu d'activit√©, c'est anormal
          if (timeSinceLastActivity > 800) {
            console.log("Watchdog: D√©tection d'un arr√™t anormal de la synth√®se vocale, tentative de reprise");
            
            // R√©cup√©rer le segment actuel et essayer de le rejouer
            const currentIndex = activeSpeechRef.current.currentSegment;
            const totalSegments = activeSpeechRef.current.totalSegments;
            
            if (currentIndex < totalSegments) {
              console.log(`Watchdog: Reprise du segment ${currentIndex + 1}/${totalSegments}`);
              const segmentToRead = activeSpeechRef.current.segments[currentIndex];
              
              // Cr√©er une nouvelle utterance pour ce segment
              try {
                const utterance = new SpeechSynthesisUtterance(segmentToRead);
                utterance.lang = 'fr-FR';
                utterance.rate = voicePrefs.rate;
                utterance.volume = 1.0;
                
                // R√©cup√©rer une voix fran√ßaise si disponible
                try {
                  const voices = speechSynthesisRef.current.getVoices();
                  const frenchVoice = voices.find(voice => voice.lang.includes('fr'));
                  if (frenchVoice) {
                    utterance.voice = frenchVoice;
                  }
                } catch (e) {
                  console.warn("Watchdog: Impossible de d√©finir la voix", e);
                }
                
                // Enregistrer l'heure de la derni√®re activit√©
                activeSpeechRef.current.lastActivity = Date.now();
                
                // Lire le segment d'urgence
                speechSynthesisRef.current.cancel(); // Annuler tout ce qui pourrait √™tre en cours
                speechSynthesisRef.current.speak(utterance);
              } catch (err) {
                console.error("Watchdog: Erreur lors de la tentative de reprise", err);
              }
            } else {
              console.log("Watchdog: Tous les segments ont √©t√© lus, arr√™t de la surveillance");
              clearInterval(watchdogTimer);
            }
          }
        } else if (speechSynthesisRef.current.speaking) {
          // Si la synth√®se est active, mettre √† jour l'heure de la derni√®re activit√©
          activeSpeechRef.current.lastActivity = Date.now();
        }
      };
      
      // Lancer le watchdog toutes les 500ms
      watchdogTimer = setInterval(watchdog, 500);
      
      // Nettoyer le watchdog quand l'√©tat change
      return () => {
        if (watchdogTimer) {
          clearInterval(watchdogTimer);
        }
      };
    }
  }, [isSpeakingRef.current, cancelRequestedRef.current, isSpeechSynthesisSupported, voicePrefs.rate]);

  // Fonction pour le mode production √©crite avec processus en plusieurs √©tapes
  const activateIntegratedProductionMode = () => {
    // Commencer par proposer des sujets, pas directement le planificateur
    const timestamp = new Date().toLocaleTimeString([], { hour: '2-digit', minute: '2-digit' });
    
    // Ajouter un message utilisateur
    const userMessage = {
      id: messages.length + 1,
      role: 'user',
      content: "Je souhaite faire une production √©crite. Aide-moi √† choisir un sujet et √† d√©velopper mon texte.",
      timestamp: timestamp
    };
    
    // Ajouter l'instruction syst√®me pour le mode production
    const systemMessage = {
      id: messages.length + 2,
      role: 'system',
      content: "L'√©l√®ve commence une production √©crite. Guide-le par √©tapes : 1) Propose plusieurs sujets adapt√©s √† son niveau, 2) Une fois le sujet choisi, sugg√®re d'utiliser le plan d'√©criture ou propose un plan simple, 3) Aide √† d√©velopper chaque partie avec des conseils pr√©cis et des suggestions pour enrichir le vocabulaire.",
      timestamp: timestamp
    };
    
    // Mettre √† jour les messages
    setMessages(prev => [...prev, userMessage]);
    
    // Simuler une r√©ponse de l'assistant proposant des sujets
    setIsLoading(true);
    setTimeout(() => {
      const assistantMessage = {
        id: messages.length + 3,
        role: 'assistant',
        content: "Je vais t'aider √† cr√©er une belle production √©crite ! Commen√ßons par choisir un sujet int√©ressant. Voici quelques id√©es :\n\n1. **Une aventure √† la plage** - Tu pourrais raconter une journ√©e exceptionnelle avec une d√©couverte surprenante.\n\n2. **Mon ami(e) imaginaire** - D√©cris comment est cet ami, ce que vous faites ensemble.\n\n3. **Un voyage dans le futur** - Imagine comment sera le monde dans 100 ans.\n\n4. **Le jour o√π j'ai aid√© quelqu'un** - Raconte une histoire o√π tu as fait une bonne action.\n\n5. **Un animal extraordinaire** - Invente un animal avec des pouvoirs sp√©ciaux.\n\nQuel sujet te plairait le plus ? Tu peux aussi proposer ton propre sujet si tu as une autre id√©e !",
        timestamp: new Date().toLocaleTimeString([], { hour: '2-digit', minute: '2-digit' })
      };
      
      setMessages(prev => [...prev, assistantMessage]);
      setIsLoading(false);
    }, 500);
  };
  
  // Fonction pour activer le planificateur d'√©criture
  const toggleWritingPlanner = () => {
    setShowWritingPlanner(!showWritingPlanner);
    // Masquer les modes sp√©cifiques si le planificateur est ouvert
    if (!showWritingPlanner) {
      // Si on ouvre le planificateur, on peut ajouter un message de guidage
      setMessages(prevMessages => [
        ...prevMessages,
        {
          id: Date.now(),
          role: 'assistant',
          content: "Le planificateur de production √©crite va t'aider √† organiser ton texte. Tu pourras d√©finir l'√©tat initial et la suite des √©v√©nements de ton r√©cit. Une fois termin√©, le plan sera envoy√© dans notre conversation et je pourrai t'aider √† d√©velopper ton texte et √† l'am√©liorer.",
          timestamp: new Date().toLocaleTimeString([], { hour: '2-digit', minute: '2-digit' })
        }
      ]);
    }
  };

  // Fonction pour le mode correction
  const activateCorrectionMode = () => {
    // Envoyer directement une demande de correction
    const timestamp = new Date().toLocaleTimeString([], { hour: '2-digit', minute: '2-digit' });
    
    // Ajouter un message utilisateur
    const userMessage = {
      id: messages.length + 1,
      role: 'user',
      content: "Je souhaite faire corriger un texte selon les 7 crit√®res d'√©valuation.",
      timestamp: timestamp
    };
    
    // Ajouter l'instruction syst√®me pour le mode correction
    const systemMessage = {
      id: messages.length + 2,
      role: 'system',
      content: "L'√©l√®ve souhaite que tu corriges un texte selon les 7 crit√®res. Demande-lui de partager son texte. Apr√®s r√©ception du texte, analyse-le en fonction des 7 crit√®res d'√©valuation d√©taill√©s plus haut. Structure ta r√©ponse en 7 sections, une pour chaque crit√®re. Pr√©cise les points forts et les points √† am√©liorer.",
      timestamp: timestamp
    };
    
    // Mettre √† jour les messages
    setMessages(prev => [...prev, userMessage]);
    
    // Simuler une r√©ponse de l'assistant
    setIsLoading(true);
    setTimeout(() => {
      const assistantMessage = {
        id: messages.length + 3,
        role: 'assistant',
        content: "Je suis pr√™t √† corriger ton texte selon les 7 crit√®res d'√©valuation. Partage-le-moi, et je t'aiderai √† l'am√©liorer.",
        timestamp: new Date().toLocaleTimeString([], { hour: '2-digit', minute: '2-digit' })
      };
      
      setMessages(prev => [...prev, assistantMessage]);
      setIsLoading(false);
    }, 500);
  };

  // Fonction pour appliquer le template du planificateur dans le flux de conversation
  const handleApplyWritingTemplate = (template) => {
    // Cacher le planificateur
    setShowWritingPlanner(false);
    
    // Ajouter le plan directement au message en cours d'√©dition, sans envoyer automatiquement
    setNewMessage(`Voici le plan de mon texte :\n\n${template}`);
    
    // Mettre le focus sur le champ de texte
    setTimeout(() => {
      document.querySelector('.chat-input .form-control')?.focus();
    }, 100);
  };

  return (
    <div className="chat-page d-flex flex-column vh-100">
      <header className="container-fluid py-2 bg-white shadow-sm">
        <div className="container d-flex justify-content-between align-items-center">
          <div className="d-flex align-items-center">
            <Link to="/" className="btn btn-outline-secondary me-3">
              <i className="fas fa-arrow-left"></i>
            </Link>
            <h1 className="h5 mb-0">
              <span style={{ color: 'var(--primary-color)' }}>Chat</span>
              <span className="tech-accent">Alimi</span> 
              <span className="badge bg-info ms-2" style={{ fontSize: '0.6rem', verticalAlign: 'top' }}>AI</span>
            </h1>
          </div>
          <div className="badge bg-success">
            <i className="fas fa-graduation-cap me-1"></i> Assistant √âducatif
          </div>
        </div>
      </header>

      <div className="container flex-grow-1 py-4">
        <div className="chat-container p-3 d-flex flex-column mx-auto" style={{ maxWidth: '850px' }}>
          <div className="chat-messages flex-grow-1 overflow-auto p-3 mb-3" style={{ maxHeight: '70vh' }}>
            {isSpeechSynthesisSupported && (
              <div className="text-center mb-3">
                <button 
                  className="btn btn-sm btn-outline-primary me-2"
                  onClick={() => {
                    if (isSpeaking) {
                      // Utiliser la fonction d'annulation s√©curis√©e
                      cancelSpeech();
                    } else {
                      // Lire le dernier message de l'assistant
                      const assistantMessages = messages.filter(m => m.role === 'assistant');
                      if (assistantMessages.length > 0) {
                        const lastAssistantMessage = assistantMessages[assistantMessages.length - 1];
                        handleSpeakMessage(lastAssistantMessage.content, lastAssistantMessage.id);
                      }
                    }
                  }}
                  disabled={!isSpeechSynthesisSupported || messages.filter(m => m.role === 'assistant').length === 0}
                >
                  <i className={`fas ${isSpeaking ? 'fa-stop' : 'fa-volume-up'}`}></i>
                  {' '}{isSpeaking ? 'Arr√™ter la lecture' : 'Lire dernier message'}
                </button>
                
                <button 
                  className="btn btn-sm btn-outline-secondary me-2"
                  onClick={() => setShowVoiceSettings(!showVoiceSettings)}
                  title="Param√®tres de lecture vocale"
                >
                  <i className="fas fa-sliders-h"></i>{' '}
                  Options de lecture
                </button>
                
                <div className="mt-2 small text-muted">
                  <i className="fas fa-info-circle me-1"></i>
                  Si la lecture est incompl√®te, activez le "Mode fiabilit√© maximale" dans les options
                </div>
                
                {/* Param√®tres de lecture vocale */}
                {showVoiceSettings && (
                  <div className="voice-settings card p-3 mt-2 text-start">
                    <h6 className="mb-3"><i className="fas fa-cog me-2"></i>Param√®tres de lecture vocale</h6>
                    
                    <div className="mb-3">
                      <label htmlFor="voice-rate" className="form-label d-flex justify-content-between">
                        <span>Vitesse</span>
                        <span className="badge bg-primary">
                          {voicePrefs.rate < 0.8 ? 'Lente' : voicePrefs.rate > 1.1 ? 'Rapide' : 'Normale'}
                        </span>
                      </label>
                      <div className="btn-group w-100" role="group">
                        <button 
                          type="button" 
                          className={`btn btn-sm ${voicePrefs.rate < 0.8 ? 'btn-primary' : 'btn-outline-primary'}`}
                          onClick={() => setVoicePrefs({...voicePrefs, rate: 0.7})}
                        >
                          <i className="fas fa-walking"></i> Lente
                        </button>
                        <button 
                          type="button" 
                          className={`btn btn-sm ${voicePrefs.rate >= 0.8 && voicePrefs.rate <= 1.1 ? 'btn-primary' : 'btn-outline-primary'}`}
                          onClick={() => setVoicePrefs({...voicePrefs, rate: 0.9})}
                        >
                          <i className="fas fa-running"></i> Normale
                        </button>
                        <button 
                          type="button" 
                          className={`btn btn-sm ${voicePrefs.rate > 1.1 ? 'btn-primary' : 'btn-outline-primary'}`}
                          onClick={() => setVoicePrefs({...voicePrefs, rate: 1.2})}
                        >
                          <i className="fas fa-bicycle"></i> Rapide
                        </button>
                      </div>
                    </div>
                    
                    <div className="form-check form-switch mb-4">
                      <input 
                        className="form-check-input" 
                        type="checkbox" 
                        id="segmentation-switch" 
                        checked={voicePrefs.useSegmentation}
                        onChange={(e) => setVoicePrefs({...voicePrefs, useSegmentation: e.target.checked})}
                      />
                      <label className="form-check-label" htmlFor="segmentation-switch">
                        <strong>Mode fiabilit√© maximale</strong>
                        <span className="d-block small text-muted">
                          Recommand√© pour les longs textes ou si la lecture est incompl√®te
                        </span>
                      </label>
                    </div>
                    
                    <div className="text-center">
                      <button 
                        className="btn btn-outline-secondary btn-sm me-2"
                        onClick={() => {
                          setVoicePrefs({
                            rate: 0.9,
                            pitch: 1.0,
                            volume: 1.0,
                            useSegmentation: true
                          });
                        }}
                      >
                        <i className="fas fa-undo me-1"></i> R√©initialiser
                      </button>
                      
                      <button 
                        className="btn btn-primary btn-sm"
                        onClick={() => setShowVoiceSettings(false)}
                      >
                        <i className="fas fa-check me-1"></i> Appliquer
                      </button>
                    </div>
                  </div>
                )}
              </div>
            )}
            
            {messages
              .filter(message => message.role !== 'system')
              .map((message) => (
                <div 
                  key={message.id} 
                  className={`d-flex mb-3 ${message.role === 'user' ? 'justify-content-end' : 'justify-content-start'}`}
                >
                  {message.role === 'assistant' && (
                    <div className="me-2 align-self-end mb-1">
                      <div className="avatar" style={{ width: '35px', height: '35px', borderRadius: '50%', background: 'var(--primary-color)', display: 'flex', alignItems: 'center', justifyContent: 'center' }}>
                        <i className="fas fa-robot text-white"></i>
                      </div>
                    </div>
                  )}
                  <div 
                    className={`message p-3 rounded-3 ${
                      message.role === 'user' 
                        ? 'bg-primary text-white' 
                        : 'bg-light border'
                    } ${currentSpeakingMessageId === message.id ? 'currently-speaking' : ''}`}
                    style={{ maxWidth: '80%' }}
                  >
                    {message.image && (
                      <div className="message-image mb-2">
                        <img 
                          src={message.image} 
                          className="img-fluid rounded"
                          style={{ maxHeight: '200px', cursor: 'pointer' }}
                          onClick={() => window.open(message.image, '_blank')}
                          aria-label="Image partag√©e par l'utilisateur"
                        />
                      </div>
                    )}
                    
                    {/* Afficher les images provenant du mod√®le AI (si pr√©sentes) */}
                    {message.images && message.images.map((imageUrl, index) => (
                      <div className="message-image mb-2" key={`img-${message.id}-${index}`}>
                        <img 
                          src={imageUrl} 
                          className="img-fluid rounded"
                          style={{ maxHeight: '200px', cursor: 'pointer' }}
                          onClick={() => window.open(imageUrl, '_blank')}
                          aria-label={`Image ${index + 1} partag√©e par l'assistant`}
                        />
                      </div>
                    ))}
                    
                    <div 
                      className="message-text"
                      dangerouslySetInnerHTML={{ 
                        __html: formatMessage(message.content) 
                      }}
                    />
                    
                    {/* Afficher la barre de progression si le message est en cours de lecture */}
                    {currentSpeakingMessageId === message.id && (
                      <div className="speech-progress mt-2">
                        <div className="progress" style={{ height: '4px' }}>
                          <div 
                            className="progress-bar bg-accent" 
                            role="progressbar" 
                            style={{ width: `${speakingProgress}%` }} 
                            aria-valuenow={speakingProgress} 
                            aria-valuemin="0" 
                            aria-valuemax="100"
                          ></div>
                        </div>
                        <div className="speech-status text-center mt-1 small">
                          <i className="fas fa-volume-up me-1"></i> 
                          {speakingProgress < 100 ? (
                            <>Lecture en cours... <span className="badge bg-secondary">{speakingProgress}%</span></>
                          ) : (
                            <>Lecture termin√©e</>
                          )}
                        </div>
                      </div>
                    )}
                    
                    <div className="d-flex justify-content-between align-items-center mt-2">
                  <div 
                    className={`message-time small ${
                          message.role === 'user' ? 'text-light' : 'text-muted'
                        }`}
                  >
                    {message.timestamp}
                        {currentSpeakingMessageId === message.id && (
                          <span className="ms-2 speaking-indicator">
                            <i className="fas fa-volume-up fa-pulse"></i>
                            {speakingProgress > 0 && speakingProgress < 100 && (
                              <span className="ms-1">{speakingProgress}%</span>
                            )}
                          </span>
                        )}
                      </div>
                      
                      {/* Bouton pour lire le message √† haute voix (uniquement pour les messages de l'assistant) */}
                      {message.role === 'assistant' && message.content && (
                        <div className="message-actions">
                          <button 
                            className={`btn btn-sm ${isSpeaking && currentSpeakingMessageId === message.id ? 'btn-danger' : 'btn-outline-secondary'}`}
                            onClick={() => {
                              if (isSpeaking && currentSpeakingMessageId === message.id) {
                                cancelSpeech();
                              } else {
                                handleSpeakMessage(message.content, message.id);
                              }
                            }}
                            title={isSpeechSynthesisSupported ? (isSpeaking && currentSpeakingMessageId === message.id ? "Arr√™ter la lecture" : "Lire √† haute voix") : "Synth√®se vocale non support√©e"}
                            disabled={!isSpeechSynthesisSupported || (isSpeaking && currentSpeakingMessageId !== message.id)}
                          >
                            <i className={`fas ${isSpeaking && currentSpeakingMessageId === message.id ? 'fa-stop' : 'fa-volume-up'}`}></i>
                          </button>
                          
                          {/* Petits boutons pour la vitesse de lecture rapide */}
                          {!isSpeaking && (
                            <div className="btn-group btn-group-sm ms-2">
                              <button 
                                className="btn btn-outline-secondary btn-xs"
                                onClick={() => {
                                  setVoicePrefs({...voicePrefs, rate: 0.7});
                                  handleSpeakMessage(message.content, message.id);
                                }}
                                disabled={!isSpeechSynthesisSupported}
                                title="Lecture lente"
                              >
                                <i className="fas fa-walking"></i>
                              </button>
                              <button 
                                className="btn btn-outline-secondary btn-xs"
                                onClick={() => {
                                  setVoicePrefs({...voicePrefs, rate: 1.2});
                                  handleSpeakMessage(message.content, message.id);
                                }}
                                disabled={!isSpeechSynthesisSupported}
                                title="Lecture rapide"
                              >
                                <i className="fas fa-running"></i>
                              </button>
                            </div>
                          )}
                        </div>
                      )}
                    </div>
                  </div>
                  {message.role === 'user' && (
                    <div className="ms-2 align-self-end mb-1">
                      <div className="avatar" style={{ width: '35px', height: '35px', borderRadius: '50%', background: 'var(--accent-color)', display: 'flex', alignItems: 'center', justifyContent: 'center' }}>
                        <i className="fas fa-user text-white"></i>
                      </div>
                    </div>
                  )}
                </div>
              ))}
              
            {isLoading && (
              <div className="d-flex justify-content-start mb-3">
                <div className="me-2 align-self-end mb-1">
                  <div className="avatar" style={{ width: '35px', height: '35px', borderRadius: '50%', background: 'var(--primary-color)', display: 'flex', alignItems: 'center', justifyContent: 'center' }}>
                    <i className="fas fa-robot text-white"></i>
                  </div>
                </div>
                <div className="message p-3 rounded-3 bg-light border">
                  <div className="message-text">
                    <div className="typing-indicator">
                      <span></span>
                      <span></span>
                      <span></span>
                    </div>
                  </div>
                </div>
              </div>
            )}
            
            {error && (
              <div className="alert alert-danger" role="alert">
                <i className="fas fa-exclamation-triangle me-2"></i>
                {error}
              </div>
            )}
            
            <div ref={messagesEndRef} />
          </div>

          <form onSubmit={handleSend} className="chat-input">
            {imagePreview && (
              <div className="selected-image-preview mb-2 position-relative">
                <img 
                  src={imagePreview} 
                  className="img-fluid rounded message-image"
                  aria-hidden="true"
                  alt=""
                />
                <button 
                  type="button" 
                  className="btn-close position-absolute top-0 end-0 bg-danger text-white" 
                  aria-label="Supprimer" 
                  onClick={clearSelectedImage}
                  style={{ padding: '0.25rem', margin: '0.25rem' }}
                ></button>
              </div>
            )}
            
            <div className="input-group mb-2">
              {/* Suppression des boutons de mode. Le chat guidera directement l'utilisateur */}
              <button
                type="button"
                className="btn btn-outline-primary w-100"
                onClick={toggleWritingPlanner}
                disabled={isLoading}
              >
                <i className="fas fa-pencil-alt me-2"></i>
                {showWritingPlanner ? "Fermer le planificateur" : "Ouvrir le planificateur de production √©crite"}
              </button>
            </div>
            
            <div className="input-group">
              <button
                type="button"
                className="btn btn-outline-secondary btn-image"
                onClick={triggerImageSelect}
                title="Joindre une image (JPG, PNG, GIF - max 5 Mo)"
                disabled={!isFileUploadEnabled || isLoading}
              >
                <i className="fas fa-image"></i>
              </button>
              
              {/* Bouton pour activer/d√©sactiver la reconnaissance vocale */}
              <button
                type="button"
                className={`btn ${isListening ? 'btn-danger' : 'btn-outline-secondary'} btn-mic`}
                onClick={startSpeechRecognition}
                title={isSpeechRecognitionSupported ? (isListening ? "Arr√™ter l'enregistrement" : "Enregistrer un message vocal") : "Reconnaissance vocale non support√©e"}
                disabled={isLoading || !isSpeechRecognitionSupported}
              >
                <i className={`fas ${isListening ? 'fa-stop' : 'fa-microphone'}`}></i>
              </button>
              
              <input
                type="text"
                className="form-control"
                placeholder="√âcris ton message ici..."
                value={newMessage}
                onChange={(e) => setNewMessage(e.target.value)}
                disabled={isLoading || isListening}
              />
              <button 
                type="submit" 
                className="btn btn-primary"
                disabled={(newMessage.trim() === '' && !selectedImage) || isLoading || isListening}
              >
                {isLoading ? (
                  <span className="spinner-border spinner-border-sm" role="status" aria-hidden="true"></span>
                ) : (
                  <i className="fas fa-paper-plane"></i>
                )}
                {' '}Envoyer
              </button>
            </div>
            
            <input
              type="file"
              ref={fileInputRef}
              className="d-none"
              accept="image/jpeg,image/png,image/gif,image/webp"
              onChange={handleImageChange}
              capture="environment"
            />
          </form>
        </div>
        
        <div className="tips text-center mt-4">
          <div className="card p-3 border-0 shadow-sm">
            <p className="text-muted small mb-0">
              <i className="fas fa-lightbulb me-2 text-warning"></i>
              <strong>Conseil :</strong> Tu peux joindre une image √† ton message avec <i className="fas fa-image"></i>, parler avec <i className="fas fa-microphone"></i> ou √©couter les r√©ponses avec <i className="fas fa-volume-up"></i>.
            </p>
          </div>
        </div>
      </div>
      
      <footer className="py-3 bg-white border-top">
        <div className="container text-center">
          <p className="text-muted small mb-0">
            <strong>ChatAlimi</strong> - Assistant IA pour les productions √©crites | Minist√®re de l'√âducation Tunisie
          </p>
        </div>
      </footer>

      {/* Afficher le planificateur au-dessus du chat si n√©cessaire */}
      {showWritingPlanner && (
        <div className="writing-planner-overlay">
          <div className="writing-planner-modal">
            <div className="writing-planner-header d-flex justify-content-between align-items-center mb-3">
              <h3>Planificateur de production √©crite</h3>
              <button 
                className="btn-close" 
                onClick={toggleWritingPlanner}
                aria-label="Fermer"
              ></button>
            </div>
            <div className="writing-planner-content">
              <WritingPlanner onApplyTemplate={handleApplyWritingTemplate} />
            </div>
          </div>
        </div>
      )}
      
      {/* Le modal du planificateur est supprim√© car non n√©cessaire */}
    </div>
  );
};

export default ChatPage; 